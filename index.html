<!DOCTYPE html>
<html lang="en"><head>
        <meta charset="UTF-8">
        <title>Bingbing Zhuang</title>                                                            
        <meta name="description" content="Bingbing&#39;s homepage">           
            <link rel="stylesheet" href="html/bootstrap.min.css">                        
            <style>                
                .container{ 
                padding-left: 100px;
                padding-right: 15px;
                min-width: 1170px; 
                margin-right:auto;
                margin-left:auto
                } 
             </style>   
         </head>
         <body style="font-size: 16px; line-height: 1.6;">
            <div class="container">
            <div class="row">
               <div class="row">
                  <div class="col-xs-3">
                     <div style="float: left; width: 100%; padding-top: 10px;">
                        <img src="image/BingbingZhuang.jpg" alt="Portrait" width=85%>
                     </div>
                  </div>
                  <div class="col-xs-8">
                     <h3>
                        Bingbing Zhuang
                     </h3>
                     <p>    
                               I am currently a computer vision and machine learning engineer at Apple, conducting research and development in 3D vision for next-generation products. Before, I was a senior researcher at NEC Laboratories America</a>,
				headed by <a href="https://cseweb.ucsd.edu/~mkchandraker/"> Manmohan Chandraker</a>. 
				I graduated from National Univeristy of Singapore with Ph.D. degree in 2019, advised by <a href="https://www.ece.nus.edu.sg/stfpage/eleclf/">Loong Fah Cheong</a> and <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>. 
				I obtained my Bachelor's degree at University of Science and Technology of China (USTC) in 2015.
                     </p>
		    <a href="https://scholar.google.com/citations?user=9M6pGaYAAAAJ&hl=en&oi=ao">[Google Scholar]</a>
                    <a href="https://www.linkedin.com/in/bingbing-zhuang/">[LinkedIn]</a>                    
                    <p></p>
                    <p> Email: <a href="mailto:bingbingzhg@gmail.com"> bingbingzhg@gmail.com </a></p>       
		     
                  </div>
               </div>
               <hr>

                <h3>Research Interest</h3>                  
                <p> 3D Computer Vision in general, 3D reconstruction and generation, Neural Rendering, 3D perception, Structure-from-Motion.
                </p>			      					      
               
               <hr>

               <h3>
                  Publications
               </h3>

            <table>
                <tbody>

		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/drive123.gif" style="width: 298px">
                        <br>
			<br>
                            </td>
                    <td>
                        
			<a href="https://arxiv.org/pdf/2412.14494"><b>Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis
of Real Vehicles</b></a>    
                        <br>
			Chuang Lin, <strong>Bingbing Zhuang</strong>, Shanlin Sun, Ziyu Jiang, Jianfei Cai, Manmohan Chandraker			
                        <br>                       	
			<em>Arxiv, 2024 </em>
                        <br>
			[<a href="https://arxiv.org/pdf/2412.14494">PDF</a>]	
			[<a href="https://clin1223.github.io/projects/Drive123/">Project Page</a>]			    
			<br>			    
                    </td>                    
                </tr>
		
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/cvpr24_lidarf.gif" style="width: 305px">
                        <br>
			<br>
                            </td>
                    <td>
                        
			<a href="https://arxiv.org/pdf/2405.00900"><b>LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes</b></a>    
                        <br>
			Shanlin Sun, <strong>Bingbing Zhuang</strong>, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker
                        <br>
                       	<em>CVPR Data-Driven Autonomous Driving Simulation Workshop, 2024  <font color="red">(Oral)</font></em>	
                        <br>		                                                
			<em>CVPR, 2024  <font color="red">(Highlight)</font></em>
                        <br>
			[<a href="https://arxiv.org/pdf/2405.00900">PDF</a>]
			[<a href="https://siwensun.github.io/lidarf-project/">Project Page</a>]
			[<a href="https://bbzh.github.io/document/cvpr2024_lidarf_bibtex.txt">bibtex</a>]
			    <br>
			    <em><p id="pdesc" style="color: #7A7B80;"> A Lidar-enhanced neural radiance field to transform drive videos into photorealistic sensor simulation testbeds. </p><em>
                    </td>                    
                </tr>

		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/cvpr24_smore.gif" style="width: 290px">
                        <br>
			<br>
                            </td>
                    <td>                       
			<a href="https://arxiv.org/pdf/2405.02781"> <b>Instantaneous Perception of Moving Objects in 3D</b></a>    
                        <br>
			Di Liu, <strong>Bingbing Zhuang</strong>, Dimitris N. Metaxas, Manmohan Chandraker			
                        <br>
                        <em>CVPR, 2024 </em>
                        <br>
			[<a href="https://arxiv.org/pdf/2405.02781">PDF</a>]					
			[<a href="https://www.youtube.com/watch?v=AAXQ4uEcUmg&t=21s">Video</a>]
			[<a href="https://bbzh.github.io/document/cvpr2024_smore_bibtex.txt">bibtex</a>]				    
			    <br>	
			   <em><p id="pdesc" style="color: #7A7B80;"> Instantaneous subtle motion detection and estimation for vehicles in street scene. </p><em> 
                    </td>                    
                </tr>
			
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/iccv23.png" style="width: 290px">
                        <br>
			<br>
                            </td>
                    <td>
                        <a href="https://arxiv.org/pdf/2308.11223.pdf"><b>LDP-FEAT: Image Features with Local Differential Privacy</b></a>
                        <br>
			Francesco Pittaluga , <strong>Bingbing Zhuang</strong>			
                        <br>
                        <em>ICCV, 2023 </em>			
                        <br>
			[<a href="https://arxiv.org/pdf/2308.11223.pdf">PDF</a>]			
			[<a href="https://bbzh.github.io/document/iccv2023bibtex.txt">bibtex</a>]
			<br>
                        <em><p id="pdesc" style="color: #7A7B80;"> Image features with differential privacy guarantee for visual localization and structure-from-motion. </p><em>
                    </td>                    
                </tr>
				
		<tr>
                    <td width="1%">
			<br>                        

                        <img src="image/cvpr23_neurocs.gif" style="width: 300px">
                        <br>
			<br>
                            </td>
                    <td>
                        <a href="https://arxiv.org/pdf/2305.17763.pdf"><b>NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization</b></a>
                        <br>
			Zhixiang Min, <strong>Bingbing Zhuang</strong>, Samuel Schulter, Buyu Liu, Enrique Dunn, Manmohan Chandrake
                        <br>
                        <em>CVPR, 2023 </em>			
                        <br>
			[<a href="https://arxiv.org/pdf/2305.17763.pdf">PDF</a>]
			[<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Min_NeurOCS_Neural_NOCS_CVPR_2023_supplemental.zip">Supplementary</a>]
			[<a href="https://www.youtube.com/watch?v=TrE4NGyc4SQ">Video Talk</a>]			
			[<a href="https://bbzh.github.io/document/cvpr2023bibtex.txt">bibtex</a>]	
			    <br>	
		       <em><p id="pdesc" style="color: #7A7B80;">Revisiting NOCS-based 3D object detection and localization in driving scenes with categorical object NeRF.</p><em>				                         
                    </td>                    
                </tr>
								 
								 
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/cvpr22a.png" style="width: 300px">
                        <br>
			<br>
                        </td>
                    <td>
                        <br>                                                
                        <a href="https://arxiv.org/pdf/2204.12667.pdf"><b>MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation </b></a>
                        <br>
			Inkyu Shin, Yi-Hsuan Tsai, <strong>Bingbing Zhuang</strong>, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, Kuk-Jin Yoon			
                        <br>
                        <em>CVPR, 2022 </em>			
                        <br>
			[<a href="https://arxiv.org/pdf/2204.12667.pdf">PDF</a>]
			[<a href="https://www.nec-labs.com/~mas/MM-TTA/">Project Page</a>]			
			[<a href="https://www.nec-labs.com/~mas/MM-TTA/resources/bibtex.txt">bibtex</a>]
			<br>
                        <em><p id="pdesc" style="color: #7A7B80;"> Test-time domain adapation for 3D semantic segmentation leveraging image-Lidar cross-modal consistency. </p><em>
                    </td>                    
                </tr>
								 
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/cvpr22b.png" style="width: 300px">
                        <br>
			<br>
                            </td>
                    <td>
                        <br>
                        <a href="https://arxiv.org/pdf/2104.06730.pdf"><b>Weakly But Deeply Supervised Occlusion-Reasoned Parametric Road Layouts </b></a>
                        <br>
			Buyu Liu, <strong>Bingbing Zhuang</strong>, Manmohan Chandraker
                        <br>
                        <em>CVPR, 2022 </em>			
                        <br>
			[<a href="https://arxiv.org/pdf/2104.06730.pdf">PDF</a>]			
			[<a href="https://bbzh.github.io/document/cvpr22bbibtex.txt">bibtex</a>]
			<br>
                        <em><p id="pdesc" style="color: #7A7B80;"> Geometric transformation creates dense occlusion-aware semantic map from compact parametric annotation, facilitating road layout estimation. </p><em>
                    </td>                    
                </tr>						 
								 
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/ICCV21.png" style="width: 300px">
                        <br>
			<br>
                            </td>
                    <td>
                        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_Cross-Modal_Contrastive_Features_for_Video_Domain_Adaptation_ICCV_2021_paper.pdf"><b>Learning Cross-Modal Contrastive Features for Video Domain Adaptation </b></a>
                        <br>
			Donghyun Kim, Yi-Hsuan Tsai, <strong>Bingbing Zhuang</strong>, Xiang Yu, Stan Sclaroff, Kate Saenko, Manmohan Chandraker			
                        <br>
                        <em>ICCV, 2021 </em>			
                        <br>
			[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_Cross-Modal_Contrastive_Features_for_Video_Domain_Adaptation_ICCV_2021_paper.pdf">PDF</a>]
			[<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Kim_Learning_Cross-Modal_Contrastive_ICCV_2021_supplemental.pdf">Supplementary</a>]			
			[<a href="https://bbzh.github.io/document/iccv21bibtex.txt">bibtex</a>]
			<br>
                        <em><p id="pdesc" style="color: #7A7B80;"> A cross-domain and cross-modal (geometry and appearance) constrastive learning framework for video domain adpatation. </p><em>
                    </td>                    
                </tr>
								 								 				  
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/CVPR21teaser.png" style="width: 300px">
                        <br>
			<br>
                            </td>
                    <td>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhuang_Fusing_the_Old_with_the_New_Learning_Relative_Camera_Pose_CVPR_2021_paper.pdf"><b>Fusing the Old with the New: Learning Relative Camera Pose with Geometry-guided Uncertainty </b></a>
                        <br>
			<strong>Bingbing Zhuang</strong>, Manmohan Chandraker						                          
                        <br>
                        <em>CVPR, 2021 <font color="red">(Oral Presentation)</font>  </em>			
                        <br>
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhuang_Fusing_the_Old_with_the_New_Learning_Relative_Camera_Pose_CVPR_2021_paper.pdf">PDF</a>]
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhuang_Fusing_the_Old_CVPR_2021_supplemental.pdf">Supplementary</a>]
			[<a href="https://www.youtube.com/watch?v=MvaBnGPB_rw">5-min Talk</a>]			
			[<a href="https://bbzh.github.io/document/cvpr21bibtex.txt">bibtex</a>]									  
			<br>
                        <em><p id="pdesc" style="color: #7A7B80;"> A principled way to fuse the geometric camera pose estimation with CNN predictions, via learning geometry-guided uncertainty, driven by a self-attention graph neural network. </p><em>
                    </td>                    
                </tr>
		
		<tr>
                    <td width="1%">
			<br>                        
                        <img src="image/eccv20slam.gif" style="width: 300px">
                        <br>                      
                            </td>
                    <td>	
			<br>
                       <a href="https://arxiv.org/pdf/2004.10681.pdf"> <b>Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction </b></a>
                        <br>
			Lokender Tiwari, Pan Ji, Quoc-Huy Tran, <strong>Bingbing Zhuang</strong>, Saket Anand, Manmohan Chandraker						                          
                        <br>
                        <em>ECCV, 2020  </em>
                        <br>
                       [<a href="https://arxiv.org/pdf/2004.10681.pdf">PDF</a>]
                       [<a href="https://lokender.github.io/projects/pseudo-rgbd-slam/">Project Page</a>]
		       [<a href="https://www.youtube.com/watch?time_continue=7&amp;v=gdY4yn0J4E0&amp;feature=emb_logo">1-min Talk</a>]
                       [<a href="https://bbzh.github.io/document/eccv20slambibtex.txt">bibtex</a>]
		 	<br>
			<em><p id="pdesc" style="color: #7A7B80;"> Geometric SLAM and self-supervised monocular CNN-depth learning can benefit each other. </p><em>
														      
                    </td>                    
                </tr>

		<tr>

                    <td width="40%">
                        <br>                        
                        <img src="image/eccv20stitching.gif" style="width: 300px">                  
                         <br>                                           
                            </td>
                    <td>
			<br>
                        <a href="https://arxiv.org/pdf/2008.09229.pdf"><b> Image Stitching and Rectification for Hand-Held Cameras </b></a>
                        <br>
                        <strong>Bingbing Zhuang</strong>, Quoc-Huy Tran.                        
                        <br>
                        <em>ECCV, 2020 </em>
                        <br>
                       [<a href="https://arxiv.org/pdf/2008.09229.pdf">PDF</a>]
		       [<a href="http://www.nec-labs.com/~mas/RS-APAP/">Project Page</a>]
		       [<a href="https://www.youtube.com/watch?v=F_yRVx9koSo">1-min Talk</a>]	               
		       [<a href="https://bbzh.github.io/document/eccv20bibtex.txt">bibtex</a>]		       
			<br>	
		       <em><p id="pdesc" style="color: #7A7B80;">Joint rolling shutter image stitching and distortion removal, by deriving a new rolling-shutter-aware homography and a minimal 5-point solver. </p><em>				   
                    </td>
                </tr>	
		
				  
              <tr>
                    <td width="40%">                    
                      <br>                        
                        <img src="image/cvpr20.png" style="width: 300px">
                        <br>
                            </td>
                    <td>
			<br>
                        <a href="https://arxiv.org/pdf/2007.00822.pdf"><b>Understanding Road Layout from Videos as a Whole </b></a>
                        <br>
                        Buyu Liu, <strong>Bingbing Zhuang</strong>, Samuel Schulter, Pan Ji, Manmohan Chandraker.                        
                        <br>
                        <em>CVPR, 2020  </em>
                        <br>
                       [<a href="https://arxiv.org/pdf/2007.00822.pdf">PDF</a>]
		       [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Liu_Understanding_Road_Layout_CVPR_2020_supplemental.zip">Supplementary</a>]                       
		       [<a href="https://bbzh.github.io/document/cvpr20bibtex.txt">bibtex</a>]		       		  		       
		       <br>
		       <em><p id="pdesc" style="color: #7A7B80;"> Learning parametric road layout from non-parametric semantic 3D reconstruction obtained by Structure-from-Motion. </p><em>
                    </td>
                </tr>

	          <tr>
                    <td width="40%">
                      <br>
                        <img src="image/cvpr19.png" style="width: 300px">                   
                         <br>
                            </td>
                    <td>
			<br>
                        <b><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhuang_Learning_Structure-And-Motion-Aware_Rolling_Shutter_Correction_CVPR_2019_paper.pdf">Learning Structure-and-Motion-Aware Rolling Shutter Correction </b></a>
                        <br>
                        <strong>Bingbing Zhuang</strong>, Quoc-Huy Tran, Pan Ji, Loong Fah Cheong, Manmohan Chandraker. 
                        <br>
                        <em>CVPR, 2019 <font color="red">(Oral Presentation)</font> </em>
                        <br>
			 [<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhuang_Learning_Structure-And-Motion-Aware_Rolling_Shutter_Correction_CVPR_2019_paper.pdf">PDF</a>]
			 [<a href="http://www.nec-labs.com/~mas/SMARSC/">Project Page</a>]
			 [<a href="https://bbzh.github.io/document/cvpr19bibtex.txt">bibtex</a>]		       		  		       						
			<br>
			<em><p id="pdesc" style="color: #7A7B80;"> Theoretical degeneracy on SfM with a rolling-shutter camera and leveraging data-driven priors through a network that learns camera motion and scene structure
 			to undistort a single rolling shutter image. </p><em>
						      
                    </td>
                </tr>
			
		<tr>
                    <td width="40%">
                      <br>
                        <img src="image/SLAM.png" style="width: 300px">                   
                        <br>
                         <br>
                            </td>
                    <td>
			<br>
                        <b><a href="https://arxiv.org/pdf/1907.13185">Degeneracy in Self-Calibration Revisited and a Deep Learning Solution for Uncalibrated SLAM </b></a>
                        <br>
                        <strong>Bingbing Zhuang</strong>, Quoc-Huy Tran, Gim Hee Lee, Loong Fah Cheong, Manmohan Chandraker. 
                        <br>
                        <em>IROS, 2019 </em>
                        <br>
			 [<a href="https://arxiv.org/pdf/1907.13185">PDF</a>]
			 [<a href="https://www.youtube.com/watch?v=cfWq9uz2Zac&amp;feature=youtu.be">YouTube Video</a>]
			 [<a href="http://www.nec-labs.com/~mas/DSCN/">Project Page</a>]		         
			 [<a href="https://bbzh.github.io/document/iros19bibtex.txt">bibtex</a>]		       		  		       											      								      
			<br>
			<em><p id="pdesc" style="color: #7A7B80;"> Theoretical degeneracy on radial distortion self-calibration in forward motion and a network to learn radial distortion parameters and camera intrinsics for SLAM. </p><em>								   
                    </td>
                </tr>  
													     
		    <tr>
                    <td width="40%">
                      <br>                        
                        <img src="image/bata.png" style="width: 300px">                                  
                        <br>                         
                            </td>
                    <td>
			<br>	
                        <a href="https://arxiv.org/pdf/1901.00643.pdf"><b>Baseline Desensitizing In Translation Averaging </b></a>
                        <br>
                        <strong>Bingbing Zhuang</strong>, Loong Fah Cheong, Gim Hee Lee
                        <br>
                        <em>CVPR, 2018  </em>
                        <br>
			 [<a href="https://arxiv.org/pdf/1901.00643.pdf">PDF</a>]
			 [<a href="https://openaccess.thecvf.com/content_cvpr_2018/Supplemental/0067-supp.pdf">Supplementary</a>]
			 [<a href="https://bbzh.github.io/document/BATA.zip">Code</a>]
			 [<a href="https://bbzh.github.io/document/cvpr18bibtex.txt">bibtex</a>]		       		  		       												    
			<br>
			Remark: This method has been integrated as a part of <a href="https://github.com/colmap/glomap">GOLMAP</a>    			
			<br>
			<em><p id="pdesc" style="color: #7A7B80;"> A baseline-insensitive bilinear objective function for translation averaging in global SfM. Theoretically revealing the underlying subtle difference that leads to the performance gap between two convex methods, LUD and Shapefit/kick. </p><em>
                    </td>
                </tr>


                <tr>
                    <td width="40%">
                        <br>                        
                        <img src="image/iccv17.gif" style="width: 300px">                                    
                            </td>
                    <td>
			<br>
                        <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhuang_Rolling-Shutter-Aware_Differential_SfM_ICCV_2017_paper.pdf"><b>Rolling-Shutter-Aware Differential SfM and Image Rectification </b></a>
                        <br>
                        <strong>Bingbing Zhuang</strong>, Loong Fah Cheong, Gim Hee Lee
                        <br>
                        <em>ICCV, 2017  </em>
                        <br>
                        [<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhuang_Rolling-Shutter-Aware_Differential_SfM_ICCV_2017_paper.pdf">PDF</a>]
			[<a href="https://openaccess.thecvf.com/content_ICCV_2017/supplemental/Zhuang_Rolling-Shutter-Aware_Differential_SfM_ICCV_2017_supplemental.pdf">Supplementary</a>]
			[<a href="https://bbzh.github.io/document/data.rar">Dataset</a>]
			[<a href="https://bbzh.github.io/document/iccv17bibtex.txt">bibtex</a>]												   
			<br>
			See  <a href="https://github.com/ThomasZiegler/RS-aware-differential-SfM">C++ Code</a>  reimplemented by Felix Graule et al. as a 3D Vision Course Project in ETH Zurich      						
			<br>			
			<em><p id="pdesc" style="color: #7A7B80;"> Develop a rolling-shutter-aware differential SfM method for depth and motion recovery, which is further leveraged to remove rolling shutter distortion.</p><em>							    
                    </td>
                </tr>                                
               
            </tbody></table>         

               <hr>
               <h3>
                Professional Services
               </h3>                                 
                <p>Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, ICRA, IROS, 3DV, AAAI, WACV, ACCV, BMVC, ICPR, ACMMM </p>
                <p>Journal Reviewer: TPAMI, TIP, RA-L </p>
	       <hr>	
                               		                            
            </div>
            </div>
             		     
</body></html>
